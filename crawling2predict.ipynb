{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d761e093",
   "metadata": {},
   "outputs": [],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from google.cloud import translate_v2 as translate\n",
    "import string\n",
    "import preprocessor as p\n",
    "import os\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GRU,SimpleRNN, Embedding\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'C:/Users/82106/Desktop/key/innate-vigil-349313-c1b81308a813.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28da4139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input keyword : pubg\n",
      "start_date (yyyy-mm-dd) : 2022-03-16\n",
      "end_date (yyyy-mm-dd) : 2022-04-13\n",
      "maximum tweet number : 100000\n"
     ]
    }
   ],
   "source": [
    "tweets_list = []\n",
    "\n",
    "keyword = input('input keyword : ')\n",
    "start_date = input('start_date (yyyy-mm-dd) : ')\n",
    "end_date = input('end_date (yyyy-mm-dd) : ')\n",
    "max = int(input('maximum tweet number : '))\n",
    "for i,tweet in enumerate(sntwitter.TwitterSearchScraper(keyword +' since:' + start_date + ' until:' + end_date).get_items()):\n",
    "    if i > max:\n",
    "        break\n",
    "    tweets_list.append([tweet.date, tweet.content])\n",
    "\n",
    "tweets_df = pd.DataFrame(tweets_list, columns=['Datetime', 'Text'])\n",
    "tweets_df['Datetime'] = tweets_df['Datetime'].apply(lambda a: pd.to_datetime(a).date())\n",
    "\n",
    "# 크롤링 파일 저장\n",
    "corpus_file = keyword + '_' + start_date +'_' + end_date + '_tweetdata.csv'\n",
    "tweets_df.to_csv(corpus_file, encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f3f6a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = translate.Client()\n",
    "\n",
    "# 전처리 함수 내 사용\n",
    "contractions = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\", \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\", \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\", \"&gt;\" : \"is greater than\", \"&lt;\" : \"is less than\", \"&nbsp;\": \" \", \"&amp;\" : \"and\", \"&quot;\" : '\"'}\n",
    "\n",
    "def preprocessing(data) :\n",
    "    data = re.sub('’|‘|´|`', '\\'', data)\n",
    "    data = re.sub('\\\\n', ' ', data)\n",
    "    # normalize abbreviation\n",
    "    data = ' '.join([contractions[t] if t in contractions else t for t in data.split(\" \")])\n",
    "    # clean corpus to remove emoji and URL\n",
    "    data = p.clean(data)\n",
    "    # remove numbers and punctuations\n",
    "    data = re.sub(r'[^A-Za-z ]', ' ', data)\n",
    "    # lowering\n",
    "    data = data.lower()\n",
    "    # remove duplicated alphabets\n",
    "    data = re.sub(r'([A-Za-z])\\1{2,}', r'\\1', data)\n",
    "    \n",
    "    data = re.sub('league of legends', 'leagueoflegends', data)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def is_english(data) :\n",
    "    if len(data) < 2: return ''\n",
    "    if client.detect_language(data)['language'] == 'en': return data\n",
    "    else : return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79c60a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 원본 파일 읽어옴\n",
    "df = pd.read_csv(corpus_file, encoding=\"UTF-8\")\n",
    "df.drop_duplicates(subset=['Text'], inplace=True)\n",
    "\n",
    "# 전처리\n",
    "df['Cleaned'] = df['Text'].apply(lambda x : preprocessing(x))\n",
    "# 영어만 남김\n",
    "df['Cleaned'] = df['Cleaned'].apply(lambda x : is_english(x))\n",
    "# 불필요한 공백, 없어진 문장 처리\n",
    "df_temp = df[df['Cleaned'] == ''].index\n",
    "df = df.drop(df_temp)\n",
    "result = df[['Datetime', 'Cleaned']].rename(columns={'Cleaned':'Text'})\n",
    "result['Text'] = result['Text'].apply(lambda x : ' '.join(x.split()))\n",
    "# 불용어 처리\n",
    "stop_words = set(stopwords.words('english')) \n",
    "result['Clean'] = result['Text'].apply(lambda x : ' '.join(word for word in x.split() if not word in stop_words if len(word) > 1))\n",
    "# 없어진 문장 처리\n",
    "result_temp = result[result['Clean'] == ''].index\n",
    "result = result.drop(result_temp)\n",
    "# 파일로 저장\n",
    "result.reset_index(drop=True).set_index('Datetime').to_csv(corpus_file + '_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57371050",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_predict(new_sentence):\n",
    "    # 알파벳과 숫자를 제외하고 모두 제거 및 알파벳 소문자화\n",
    "    encoded = []\n",
    "    # 띄어쓰기 단위 토큰화 후 정수 인코딩\n",
    "    for word in new_sentence.split():\n",
    "        try :\n",
    "            # 단어 집합의 크기를 10,000으로 제한.\n",
    "            if word_to_index[word] < 10000 - 3:\n",
    "                encoded.append(word_to_index[word]+3)\n",
    "            else:\n",
    "                # 10,000 이상의 숫자는 <unk> 토큰으로 변환.\n",
    "                encoded.append(2)\n",
    "        # 단어 집합에 없는 단어는 <unk> 토큰으로 변환.\n",
    "        except KeyError:\n",
    "            encoded.append(2)\n",
    "\n",
    "    pad_sequence = pad_sequences([encoded], maxlen=max_len)\n",
    "    score = float(loaded_model.predict(pad_sequence)) # 예측\n",
    "    \n",
    "    if(score > 0.65) : return 1\n",
    "    elif(score > 0.35) : return 0\n",
    "    else : return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8ec3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 500\n",
    "loaded_model = load_model('GRU_model.h5')\n",
    "index_to_word = {}\n",
    "word_to_index = imdb.get_word_index()\n",
    "for key, value in word_to_index.items():\n",
    "    index_to_word[value+3]=key\n",
    "for index, token in enumerate((\"<pad>\",\"<sos>\",\"<unk>\")):\n",
    "    index_to_word[index]=token\n",
    "\n",
    "# 파일 읽음\n",
    "df = pd.read_csv(corpus_file + '_preprocessed.csv', encoding='UTF-8')\n",
    "\n",
    "# 감성 분석\n",
    "df['Label'] = df['Text'].apply(lambda x : sentiment_predict(x))\n",
    "\n",
    "# 파일 저장\n",
    "df.set_index(\"Datetime\").to_csv(corpus_file + \"_lable.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
